{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Exploratory Data Analysis (EDA): Under the Hood of Legal AI\n",
    "\n",
    "Welcome to the **Research Lab**. As an AI Architect, the first step isn't coding‚Äîit's understanding the data. This notebook bridges the gap between raw legal text and a high-performance machine learning model.\n",
    "\n",
    "## üéì AI Foundations: What are we doing?\n",
    "\n",
    "### 1. Traditional Programming vs. AI\n",
    "- **Traditional Programming**: We write rules (`if word == \"liability\": risk = high`). But contracts are too complex for millions of `if` statements.\n",
    "- **AI/Machine Learning**: We show the computer 20,000 examples of \"Risk\" vs \"No Risk\" and let it develop its own mathematical intuition.\n",
    "\n",
    "### 2. The Classification Task\n",
    "We are performing **Binary Classification**. Our target variable is `clause_status`:\n",
    "- `0`: Low Risk (Standard operational language)\n",
    "- `1`: High Risk (Obligations, liabilities, and legal triggers)\n",
    "\n",
    "### 3. NLP Fundamentals: TF-IDF\n",
    "Computers don't \"read\"; they calculate. We use **TF-IDF (Term Frequency-Inverse Document Frequency)** to turn words into numbers. It highlights words that are frequent in a specific clause but rare across the whole contract (like \"Indemnify\"), making them strong signals for our AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Set project style - Making it look professional\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_palette(\"magma\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ 1. Data Ingestion & Health Check\n",
    "Before analysis, we must ensure our data is clean. Missing values (NaNs) are the \"kryptonite\" of Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/legal_docs_modified.csv')\n",
    "\n",
    "# --- Data Scientist Health Check ---\n",
    "print(\"üìã Dataset Shape:\", df.shape)\n",
    "print(\"\\nüîç Missing Value Analysis:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Cleaning: Removing empty clauses\n",
    "df = df.dropna(subset=['clause_text'])\n",
    "print(f\"\\n‚úÖ Cleaned Rows: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìè 2. Clause Length Analysis\n",
    "**Hypothesis**: High-risk legal clauses are often longer because they require precise, verbose language to define obligations.\n",
    "\n",
    "We calculate the length (number of characters) for every clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['char_length'] = df['clause_text'].astype(str).apply(len)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df, x='char_length', hue='clause_status', kde=True, bins=50)\n",
    "plt.title('Distribution of Clause Length by Risk Level')\n",
    "plt.xlabel('Character Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Average Length by Risk:\")\n",
    "print(df.groupby('clause_status')['char_length'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Insight**: If the distribution peaks are different, length is a valid feature for our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è 3. Risk Label Distribution\n",
    "**Theory**: In many real-world datasets, we face **Class Imbalance** (e.g., more Low Risk than High Risk). If the AI only sees Low Risk, it won't recognize danger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#66b3ff', '#ff9999']\n",
    "plt.figure(figsize=(8, 8))\n",
    "df['clause_status'].value_counts().plot.pie(autopct='%1.1f%%', colors=colors, explode=[0, 0.1])\n",
    "plt.title('Risk Proportion (0: Low, 1: High)')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è 4. Feature Importance: The Power of Words\n",
    "Using TF-IDF, we can identify which words \"own\" certain risk categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_tfidf_features(risk_level, n=15):\n",
    "    subset = df[df['clause_status'] == risk_level]['clause_text']\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "    tfidf_matrix = vectorizer.fit_transform(subset)\n",
    "    \n",
    "    # Sum weights across all documents\n",
    "    weights = tfidf_matrix.sum(axis=0).A1\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Create sorted dataframe\n",
    "    result = pd.DataFrame({'word': feature_names, 'score': weights})\n",
    "    return result.sort_values(by='score', ascending=False).head(n)\n",
    "\n",
    "print(\"üî• Top Signal Words for HIGH RISK (1):\")\n",
    "print(get_top_tfidf_features(1))\n",
    "\n",
    "print(\"\\nüåø Top Signal Words for LOW RISK (0):\")\n",
    "print(get_top_tfidf_features(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ 5. Unsupervised Discovery (Clustering Demo)\n",
    "**Theory**: clustering is **Unsupervised Learning**. We don't tell the AI what the labels are; we just ask it to find similar groups. This helps us see if legal clauses naturally separate into their types (e.g., Termination vs Liability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample data for speed\n",
    "sample_df = df.sample(1000, random_state=42)\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "X = vectorizer.fit_transform(sample_df['clause_text'])\n",
    "\n",
    "# Cluster into 3 groups\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Reduce to 2 dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "scatter_data = pca.fit_transform(X.toarray())\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(scatter_data[:, 0], scatter_data[:, 1], c=clusters, cmap='viridis', alpha=0.6)\n",
    "plt.title('Unsupervised Visual: 3 Natural Groupings in Legal Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÅ Conclusion & Next Steps\n",
    "\n",
    "Through this EDA, we've confirmed:\n",
    "1. Our data has **class imbalance**, so we need balanced model weights.\n",
    "2. **Feature separation** exists (high risk has different keywords than low risk).\n",
    "3. **Unsupervised clusters** show that legal text naturally groups into patterns.\n",
    "\n",
    "Now, we are ready to teach our **brain** using Logistic Regression!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
